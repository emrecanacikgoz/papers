# Multimodal
papers, notes and codes

r221008:1908:Microsoft:VL-BERT:Pre-training of Generic Visual-Linguistic Representations.pdf. 

VL-BERT takes both visual and linguistic embedded features as input. Each input element is either of a word from the input sentence, or a region-of-interest (RoI) from the input image, together with certain special elements to disambiguate different input formats. ViLBERT and LXMERT are based on two single-modal (two-stream designed) networks applied on input sentences and images respectively. A cross-modal Transformer is used to combine the information from the two sources. It is of a unified architecture based on Transformers without any restriction on the attention patterns. The visual and linguistic contents are fed as input to VL-BERT and it outperforms such two-stream designs.

(1) Sentence-Image Relationship Prediction used in ViLBERT and LXMERT does not help in pre-training visual-linguistic representations. (2) VL-BERT is pretrained both on visual-linguistic (Conceptual Captions Dataset) and text-only (Books Corpus and English Wikipedia) datasets since joint pre-training improves the generalization over long and complex sentences. (3) Improved tuning of the visual representation. In VL-BERT, the parameters of Fast R-CNN are also updated. To avoid visual clue leakage in the pre-training task of Masked RoI Classification with Linguistic Clues, the masking operation is applied to the input raw pixels, other than the feature maps produced by layers of convolution.


For each input element, its embedding feature is the summation of four types of embedding: token embedding, visual feature embedding, segment embedding, and sequence position embedding. The visual feature embedding is newly introduced for capturing visual clues, while the other three embeddings follow the design in the original BERT paper.


=> Token Embedding: a special [IMG] token is assigned for each one of them.
=> Visual Feature Embedding: Visual appearance feature and visual geometry embedding are combined to form the visual feature embedding. The visual appearance feature is extracted by applying a Fast R-CNN detector where the feature vector prior to the output layer of each RoI is utilized as the visual feature embedding (of 2048-d in paper). The visual geometry embedding is designed to inform VL-BERT the geometry location of each input visual element in image. Each RoI is characterized by a 4-d vector, as ( xLT , yLT , xRB , hRB ). Following the practice in Relation Networks (Hu et al., 2018), the 4-d vector is embedded into a high-dimensional representation (of 2048-d in paper) by computing sine and cosine functions of different wavelengths. The visual feature embedding is added to all the inputs, which is an output of a fully connected layer taking the concatenation of visual appearance feature and visual geometry embedding as input.
=> Segment Embedding:  A learned segment embedding is added to every input element for indicating which segment it belongs to. There are three types of segments in VL-BERT: A, B, C. They are defined to separate input elements from different sources. A and B are defined for the words from the first and second input sentence, and C for the RoIs from the input image. For QA the format is <Question, Answer, Image> where A denotes Question, B denotes Answer, and C denotes Image. For Image-Caption task, the input format is <Caption, Image> where A denotes Caption, and C denotes Image.
=> Sequence Position Embedding: Same in BERT. Each input element indicatrd its order in the input sequence and the learnable sequence position embedding is added to every input element indicating its order in the input sequence. The sequence position embedding for all visual elements are the same since there is no natural order among input visual elements.

VL-BERT is pretrained on Conceptual Captions Dataset as the visual-linguistic corpus. The captions of CC is clauses that are too simple and short which leads overfitting. So, VL-BERT is also pretrained on text-only Books Corpus and the English Wikipedia datasets. 

VL-BERT is pre-trained on two tasks: (1) Masked Language Modeling with Visual Clues: where visual clueas are added to make the alignment between viusal and linguistic contents. Each word in the input sentence is randomly masked, with a special [MASK] token, with the probability of 15% during pretraining. The model is trained to predict the masked words, based on the unmasked words and the visual features. (2) Masked RoI Classification with Linguistic Clues: To avoid any visual clue leakage from the visual feature embedding of other elements, the pixels laid in the masked RoI are set as zeros before applying Fast R-CNN. During pre-training, the final output feature corresponding to the masked RoI is fed into a classifier with Softmax cross-entropy loss for object category classification. The category label predicted by pre-trained Faster R-CNN is set as the ground-truth.

r221031:1908:UCLA:VisualBERT:A Simple and Performant Baseline for Vision and Language.pdf
